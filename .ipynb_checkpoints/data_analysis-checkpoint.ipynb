{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from jieba import posseg\n",
    "import numpy as np\n",
    "from loader import *\n",
    "import torch\n",
    "import sys\n",
    "from modules import embedding, encoder, model_sbj\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'postag': [{'word': '如何', 'pos': 'r'}, {'word': '演', 'pos': 'v'}, {'word': '好', 'pos': 'a'}, {'word': '自己', 'pos': 'r'}, {'word': '的', 'pos': 'u'}, {'word': '角色', 'pos': 'n'}, {'word': '，', 'pos': 'w'}, {'word': '请', 'pos': 'v'}, {'word': '读', 'pos': 'v'}, {'word': '《', 'pos': 'w'}, {'word': '演员自我修养', 'pos': 'nw'}, {'word': '》', 'pos': 'w'}, {'word': '《', 'pos': 'w'}, {'word': '喜剧之王', 'pos': 'nw'}, {'word': '》', 'pos': 'w'}, {'word': '周星驰', 'pos': 'nr'}, {'word': '崛起', 'pos': 'v'}, {'word': '于', 'pos': 'p'}, {'word': '穷困潦倒', 'pos': 'a'}, {'word': '之中', 'pos': 'f'}, {'word': '的', 'pos': 'u'}, {'word': '独门', 'pos': 'n'}, {'word': '秘笈', 'pos': 'n'}], 'text': '如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈', 'spo_list': [{'predicate': '主演', 'object_type': '人物', 'subject_type': '影视作品', 'object': '周星驰', 'subject': '喜剧之王'}]}\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/train_data.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        tmp = json.loads(line)\n",
    "        print(tmp)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.435 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spo make samples, nums:191739/196864, radio:0.9740\n"
     ]
    }
   ],
   "source": [
    "a = gen_train_data_spo(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open('../data/p_dict.pkl', 'rb') as f:\n",
    "        i2s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p2i': {'丈夫': 1,\n",
       "  '歌手': 2,\n",
       "  '占地面积': 3,\n",
       "  '邮政编码': 4,\n",
       "  '国籍': 5,\n",
       "  '出品公司': 6,\n",
       "  '出版社': 7,\n",
       "  '注册资本': 8,\n",
       "  '上映时间': 9,\n",
       "  '民族': 10,\n",
       "  '目': 11,\n",
       "  '所在城市': 12,\n",
       "  '创始人': 13,\n",
       "  '成立日期': 14,\n",
       "  '祖籍': 15,\n",
       "  '导演': 16,\n",
       "  '制片人': 17,\n",
       "  '气候': 18,\n",
       "  '修业年限': 19,\n",
       "  '连载网站': 20,\n",
       "  '主持人': 21,\n",
       "  '父亲': 22,\n",
       "  '主演': 23,\n",
       "  '人口数量': 24,\n",
       "  '总部地点': 25,\n",
       "  '简称': 26,\n",
       "  '作词': 27,\n",
       "  '作者': 28,\n",
       "  '首都': 29,\n",
       "  '号': 30,\n",
       "  '董事长': 31,\n",
       "  '所属专辑': 32,\n",
       "  '朝代': 33,\n",
       "  '专业代码': 34,\n",
       "  '毕业院校': 35,\n",
       "  '嘉宾': 36,\n",
       "  '海拔': 37,\n",
       "  '出生地': 38,\n",
       "  '出生日期': 39,\n",
       "  '妻子': 40,\n",
       "  '字': 41,\n",
       "  '身高': 42,\n",
       "  '作曲': 43,\n",
       "  '主角': 44,\n",
       "  '官方语言': 45,\n",
       "  '面积': 46,\n",
       "  '改编自': 47,\n",
       "  '母亲': 48,\n",
       "  '编剧': 49},\n",
       " 'i2p': {1: '丈夫',\n",
       "  2: '歌手',\n",
       "  3: '占地面积',\n",
       "  4: '邮政编码',\n",
       "  5: '国籍',\n",
       "  6: '出品公司',\n",
       "  7: '出版社',\n",
       "  8: '注册资本',\n",
       "  9: '上映时间',\n",
       "  10: '民族',\n",
       "  11: '目',\n",
       "  12: '所在城市',\n",
       "  13: '创始人',\n",
       "  14: '成立日期',\n",
       "  15: '祖籍',\n",
       "  16: '导演',\n",
       "  17: '制片人',\n",
       "  18: '气候',\n",
       "  19: '修业年限',\n",
       "  20: '连载网站',\n",
       "  21: '主持人',\n",
       "  22: '父亲',\n",
       "  23: '主演',\n",
       "  24: '人口数量',\n",
       "  25: '总部地点',\n",
       "  26: '简称',\n",
       "  27: '作词',\n",
       "  28: '作者',\n",
       "  29: '首都',\n",
       "  30: '号',\n",
       "  31: '董事长',\n",
       "  32: '所属专辑',\n",
       "  33: '朝代',\n",
       "  34: '专业代码',\n",
       "  35: '毕业院校',\n",
       "  36: '嘉宾',\n",
       "  37: '海拔',\n",
       "  38: '出生地',\n",
       "  39: '出生日期',\n",
       "  40: '妻子',\n",
       "  41: '字',\n",
       "  42: '身高',\n",
       "  43: '作曲',\n",
       "  44: '主角',\n",
       "  45: '官方语言',\n",
       "  46: '面积',\n",
       "  47: '改编自',\n",
       "  48: '母亲',\n",
       "  49: '编剧'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.449 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbj, make samples_num:172333, sbj_nums:195042/196864, radio:0.9907\n"
     ]
    }
   ],
   "source": [
    "def gen_train_data_sbj(file_path):\n",
    "    sbj_dict = get_type_dict_sbj()\n",
    "    texts = []\n",
    "    text_lists = []\n",
    "    tag_lists = []\n",
    "    spo_lists = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            tmp = json.loads(line)\n",
    "            texts.append(tmp['text'])\n",
    "            text_list, tag_list = list(zip(*posseg.lcut(tmp['text'], HMM=False)))\n",
    "            text_list = list(text_list)\n",
    "            tag_list = list(tag_list)\n",
    "            text_lists.append(text_list)\n",
    "            tag_lists.append(tag_list)\n",
    "            spo_lists.append(tmp['spo_list'])\n",
    "\n",
    "    with open('../data/sbj_dict.pkl', 'rb') as f:\n",
    "        sbj2i = pickle.load(f)['sbj2i']\n",
    "\n",
    "    r_text_lists = []\n",
    "    r_tag_lists = []\n",
    "    r_sbjs = []\n",
    "    nums = 0\n",
    "    for text_list, tag_list, spo_list in zip(text_lists, tag_lists, spo_lists):\n",
    "        text_len = len(text_list)\n",
    "        sbj = np.zeros(text_len)\n",
    "\n",
    "        spo_extract = set()\n",
    "        for spo in spo_list:\n",
    "            sbj_list, _ = list(zip(*posseg.lcut(spo['subject'], HMM=False)))\n",
    "            sbj_list = list(sbj_list)\n",
    "            sbj_len = len(sbj_list)\n",
    "            for i in range(0, text_len-sbj_len+1):\n",
    "                if text_list[i: i+sbj_len] == sbj_list:\n",
    "                    sbj_s = i\n",
    "                    sbj_e = i + sbj_len - 1\n",
    "                    xxx = sbj_dict[spo['subject_type']]\n",
    "                    if sbj_s == sbj_e:\n",
    "                        sbj[sbj_s] = sbj2i[xxx]\n",
    "                    elif sbj_e - sbj_s == 1:\n",
    "                        sbj[sbj_s] = sbj2i[xxx]\n",
    "                        sbj[sbj_e] = sbj2i[xxx] + 2\n",
    "                    elif sbj_e - sbj_s > 1:\n",
    "                        sbj[sbj_s] = sbj2i[xxx]\n",
    "                        sbj[sbj_s+1: sbj_e] = sbj2i[xxx] + 1\n",
    "                        sbj[sbj_e] = sbj2i[xxx] + 2\n",
    "                    else:\n",
    "                        print('wrong')\n",
    "                        assert 1 == -1\n",
    "                    spo_extract.add(spo['subject'])\n",
    "        nums += len(spo_extract)\n",
    "        if len(spo_extract) != 0:\n",
    "            r_text_lists.append(text_list)\n",
    "            r_tag_lists.append(tag_list)\n",
    "            r_sbjs.append(sbj.tolist())\n",
    "\n",
    "    all_nums = 0\n",
    "    for spo_list in spo_lists:\n",
    "        all_nums += len(set([spo['subject'] for spo in spo_list]))\n",
    "\n",
    "    print('sbj, make samples_num:%d, sbj_nums:%d/%d, radio:%.4f' % (len(r_text_lists), nums, all_nums, nums/all_nums))\n",
    "\n",
    "    return r_text_lists, r_tag_lists, r_sbjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a[2][23]我靠dsaas'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a[2][23]我靠dsaAS'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbj, make samples_num:173109, sbj_nums:361242/364218, radio:0.9918\n"
     ]
    }
   ],
   "source": [
    "spo_lists = spo_list\n",
    "all_nums = 0\n",
    "for spo_list in spo_lists:\n",
    "    all_nums += len([spo['subject'] for spo in spo_list])\n",
    "\n",
    "print('sbj, make samples_num:%d, sbj_nums:%d/%d, radio:%.4f' % (len(texts), nums, all_nums, nums/all_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [jieba.lcut(item['word'], HMM=False) for item in tmp['postag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
